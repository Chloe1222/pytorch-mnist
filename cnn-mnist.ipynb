{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST digit recognizer with pytorch and Kaggle\n",
    "\n",
    "This notebook develops a convolutional neural network with pytorch to identify hand written digits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/envs/pytorch/lib/python3.6/site-packages/matplotlib/font_manager.py:280: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the data\n",
    "\n",
    "Before starting, we must download the data from Kaggle.\n",
    "\n",
    "We'll store it in a subdirectory; create a `data/` directory if it does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, download the training and test data sets if they do not already exist locally, ensuring that they end up in the `data/` directory. The data are available here: https://www.kaggle.com/c/digit-recognizer/data \n",
    "\n",
    "## Munging the data\n",
    "\n",
    "We need to load the data and produce ndarrays for the training data (divided into a training set and validation set), which can be converted to pytorch tensors, where the inputs are 4d, with dimensions corresponding to 1) sample size, 2) number of channels, 3) width, and 4) height. The labels will be 1 dimensional integer vectors which indicate whether a handwritten digit is a \"0\", \"1\", ..., or \"9\". \n",
    "\n",
    "Loading the training data set with pandas, which has one row per example, and the columns represent unrolled pixel intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, extract numeric values which represent intensities (excluding column 0), compute the maximum value for use in normalization later, and jot down the total number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgs = df.iloc[:, 1:].values # iloc: integer indexing, values: numpy ndarray\n",
    "max_imgs = np.max(imgs)\n",
    "mean_imgs = np.mean(imgs)\n",
    "sd_imgs = np.std(imgs)\n",
    "ntot = imgs.shape[0]         # number of rows\n",
    "\n",
    "imgs = imgs / max_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into a 60/40 training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrain = np.floor(ntot * .6)\n",
    "train_indices = np.random.randint(0, int(ntot), size = int(ntrain))\n",
    "\n",
    "nvalid = ntot - ntrain\n",
    "valid_indices = np.setdiff1d(list(range(0, ntot)), train_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cubing the training data\n",
    "\n",
    "We've got to un-unroll the data into a 4d numpy array and create our vector of labels, divided into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a ntrain * 28 * 28 ndarray of training set\n",
    "cube = np.ndarray(shape = (ntot, 1, 28, 28), dtype = \"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0, ntot):\n",
    "    cube[i, 0, :, :] = imgs[i, :].reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 27182695.,    399808.,    359330.,    306344.,    312800.,\n",
       "           363558.,    317852.,    362706.,    427725.,   2895182.]),\n",
       " array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADe1JREFUeJzt3X+o3fV9x/HnayaODWWW5W5KTHpdydrp0OkyFVtK1jFm\nrBAGDuKKMicEnS0W+kfFP/SP/WP/6Tab1hDaIEJRWBWX0dgitJ2WVudNiNEk2GW2q3EBU12Tpgol\n870/zrG73N7kfG/u+XHvJ88HXDznfD/3nPeHyNPj937vSaoKSVJbfm3SA0iShs+4S1KDjLskNci4\nS1KDjLskNci4S1KDJhr3JDuSvJHk5Q5r/yHJ3v7XD5L8dBwzStJylEle557ko8AJ4JGq+sMFfN+n\ngCur6m9HNpwkLWMTfedeVc8Ab81+LMkHknwjye4kzyb50DzfejPw6FiGlKRlaMWkB5jHduCOqvqP\nJNcAXwI+9t7BJO8HLgG+NaH5JGnJW1JxT3IecB3wz0nee/jX5yzbDHytqv53nLNJ0nKypOJO7zTR\nT6vqj06zZjNw15jmkaRlaUldCllVx4EfJvkrgPRc8d7x/vn39wHfn9CIkrQsTPpSyEfphfqDSQ4n\nuR34BHB7kheB/cCmWd+yGXis/ChLSTqtiV4KKUkajSV1WkaSNBwT+4HqqlWranp6elIvL0nL0u7d\nu39SVVOD1k0s7tPT08zMzEzq5SVpWUryX13WeVpGkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZd\nkhpk3CWpQcZdkhq01D7PvZPpe74+sdf+0QMfn9hrS1JXvnOXpAYZd0lqkHGXpAYZd0lqkHGXpAYZ\nd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lq0MC4J1mT5NtJDiTZn+TuedZsSHIsyd7+132j\nGVeS1EWXT4U8CXymqvYkOR/YneTpqjowZ92zVXXj8EeUJC3UwHfuVXWkqvb0b/8MOAisHvVgkqQz\nt6Bz7kmmgSuB5+c5fF2SfUmeSnLZKb5/S5KZJDNHjx5d8LCSpG46xz3JecDjwKer6vicw3uAtVV1\nOfAF4Mn5nqOqtlfV+qpaPzU1daYzS5IG6BT3JCvphf2rVfXE3ONVdbyqTvRv7wJWJlk11EklSZ11\nuVomwFeAg1X1+VOsubC/jiRX95/3zWEOKknqrsvVMh8GbgFeSrK3/9i9wFqAqtoG3ATcmeQk8A6w\nuapqBPNKkjoYGPeq+i6QAWu2AluHNZQkaXH8DVVJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJ\napBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBx\nl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJatDAuCdZk+TbSQ4k2Z/k\n7nnWJMmDSQ4l2ZfkqtGMK0nqYkWHNSeBz1TVniTnA7uTPF1VB2at2Qis639dAzzU/6ckaQIGvnOv\nqiNVtad/+2fAQWD1nGWbgEeq5znggiQXDX1aSVInCzrnnmQauBJ4fs6h1cBrs+4f5lf/A0CSLUlm\nkswcPXp0YZNKkjrrHPck5wGPA5+uquNn8mJVtb2q1lfV+qmpqTN5CklSB53inmQlvbB/taqemGfJ\n68CaWfcv7j8mSZqALlfLBPgKcLCqPn+KZTuBW/tXzVwLHKuqI0OcU5K0AF2ulvkwcAvwUpK9/cfu\nBdYCVNU2YBdwA3AIeBu4bfijSpK6Ghj3qvoukAFrCrhrWENJkhbH31CVpAYZd0lqkHGXpAYZd0lq\nkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGX\npAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYN\njHuSHUneSPLyKY5vSHIsyd7+133DH1OStBArOqx5GNgKPHKaNc9W1Y1DmUiStGgD37lX1TPAW2OY\nRZI0JMM6535dkn1Jnkpy2ZCeU5J0hrqclhlkD7C2qk4kuQF4Elg338IkW4AtAGvXrh3CS0uS5rPo\nd+5VdbyqTvRv7wJWJll1irXbq2p9Va2fmppa7EtLkk5h0XFPcmGS9G9f3X/ONxf7vJKkMzfwtEyS\nR4ENwKokh4H7gZUAVbUNuAm4M8lJ4B1gc1XVyCaWJA00MO5VdfOA41vpXSopSVoi/A1VSWqQcZek\nBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3\nSWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQ\ncZekBhl3SWrQwLgn2ZHkjSQvn+J4kjyY5FCSfUmuGv6YkqSF6PLO/WHg+tMc3wis639tAR5a/FiS\npMUYGPeqegZ46zRLNgGPVM9zwAVJLhrWgJKkhRvGOffVwGuz7h/uPyZJmpCx/kA1yZYkM0lmjh49\nOs6XlqSzyjDi/jqwZtb9i/uP/Yqq2l5V66tq/dTU1BBeWpI0n2HEfSdwa/+qmWuBY1V1ZAjPK0k6\nQysGLUjyKLABWJXkMHA/sBKgqrYBu4AbgEPA28BtoxpWktTNwLhX1c0Djhdw19AmkiQtmr+hKkkN\nMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S\n1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDjLkkNMu6S1CDj\nLkkNMu6S1CDjLkkNMu6S1KBOcU9yfZJXkhxKcs88xzckOZZkb//rvuGPKknqasWgBUnOAb4I/Dlw\nGHghyc6qOjBn6bNVdeMIZpQkLVCXd+5XA4eq6tWq+gXwGLBptGNJkhajS9xXA6/Nun+4/9hc1yXZ\nl+SpJJfN90RJtiSZSTJz9OjRMxhXktTFsH6gugdYW1WXA18AnpxvUVVtr6r1VbV+ampqSC8tSZqr\nS9xfB9bMun9x/7FfqqrjVXWif3sXsDLJqqFNKUlakC5xfwFYl+SSJOcCm4GdsxckuTBJ+rev7j/v\nm8MeVpLUzcCrZarqZJJPAt8EzgF2VNX+JHf0j28DbgLuTHISeAfYXFU1wrklSacxMO7wy1Mtu+Y8\ntm3W7a3A1uGOJkk6U/6GqiQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhL\nUoM6ffyAJLVm+p6vT+y1f/TAx0f+Gr5zl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJ\napBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJalCnv2YvyfXAPwHn\nAF+uqgfmHE//+A3A28DfVNWeIc+6JEzyr+aSpK4GvnNPcg7wRWAjcClwc5JL5yzbCKzrf20BHhry\nnJKkBehyWuZq4FBVvVpVvwAeAzbNWbMJeKR6ngMuSHLRkGeVJHXU5bTMauC1WfcPA9d0WLMaODJ7\nUZIt9N7ZA5xI8sqCpv1/q4CfnOH3Llfu+ezgns8C+dyi9vz+Los6nXMflqraDmxf7PMkmamq9UMY\nadlwz2cH93x2GMeeu5yWeR1YM+v+xf3HFrpGkjQmXeL+ArAuySVJzgU2AzvnrNkJ3Jqea4FjVXVk\n7hNJksZj4GmZqjqZ5JPAN+ldCrmjqvYnuaN/fBuwi95lkIfoXQp52+hGBoZwamcZcs9nB/d8dhj5\nnlNVo34NSdKY+RuqktQg4y5JDVrScU9yfZJXkhxKcs88x5Pkwf7xfUmumsScw9Rhz5/o7/WlJN9L\ncsUk5hymQXuete5PkpxMctM45xuFLntOsiHJ3iT7k/zbuGcctg7/bv9Wkn9N8mJ/z6P+2d1IJdmR\n5I0kL5/i+Gj7VVVL8oveD2//E/g94FzgReDSOWtuAJ4CAlwLPD/pucew5+uA9/Vvbzwb9jxr3bfo\n/fD+pknPPYY/5wuAA8Da/v3fmfTcY9jzvcDn+rengLeAcyc9+yL2/FHgKuDlUxwfab+W8jv3s/Fj\nDwbuuaq+V1X/07/7HL3fKVjOuvw5A3wKeBx4Y5zDjUiXPf818ERV/Rigqpb7vrvsuYDz+x9EeB69\nuJ8c75jDU1XP0NvDqYy0X0s57qf6SIOFrllOFrqf2+n9l385G7jnJKuBv6SdD6Tr8uf8+8D7knwn\nye4kt45tutHosuetwB8A/w28BNxdVe+OZ7yJGGm/xvrxAxqeJH9KL+4fmfQsY/CPwGer6t3em7qz\nwgrgj4E/A34D+H6S56rqB5Mda6T+AtgLfAz4APB0kmer6vhkx1qelnLcz8aPPei0nySXA18GNlbV\nm2OabVS67Hk98Fg/7KuAG5KcrKonxzPi0HXZ82Hgzar6OfDzJM8AVwDLNe5d9nwb8ED1TkgfSvJD\n4EPAv49nxLEbab+W8mmZs/FjDwbuOcla4AnglkbexQ3cc1VdUlXTVTUNfA34u2Ucduj27/a/AB9J\nsiLJb9L7JNaDY55zmLrs+cf0/k+FJL8LfBB4daxTjtdI+7Vk37nX0vzYg5HquOf7gN8GvtR/J3uy\nlvEn6nXcc1O67LmqDib5BrAPeJfe34A27yV1y0HHP+e/Bx5O8hK9K0g+W1XL9qOAkzwKbABWJTkM\n3A+shPH0y48fkKQGLeXTMpKkM2TcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGvR/lQWqZcA9mqIA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc16ea91470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(cube.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a tensor containing the correct labels\n",
    "labels = df.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# break data into training and validation sets\n",
    "train_cube = torch.FloatTensor(cube[train_indices, :, :, :])\n",
    "valid_cube = torch.FloatTensor(cube[valid_indices, :, :, :])\n",
    "\n",
    "train_labels = torch.from_numpy(labels[train_indices])\n",
    "valid_labels = torch.from_numpy(labels[valid_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25200, 1, 28, 28])\n",
      "torch.Size([25200])\n",
      "torch.Size([23070, 1, 28, 28])\n",
      "torch.Size([23070])\n"
     ]
    }
   ],
   "source": [
    "print(train_cube.size())\n",
    "print(train_labels.size())\n",
    "print(valid_cube.size())\n",
    "print(valid_labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data loaders\n",
    "\n",
    "We'll use pytorch's data utilities to create loaders for the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.TensorDataset(train_cube, train_labels)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 8, \n",
    "                                          shuffle = True, num_workers = 2)\n",
    "\n",
    "testset = torch.utils.data.TensorDataset(valid_cube, valid_labels)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size = 8, \n",
    "                                          shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize some training examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8     0     6     5     1     9     1     5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABMCAYAAAB9PUwnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc16d7d9240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(' '.join('%5s'%int(labels.numpy()[j]) for j in range(trainloader.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net (\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv1_drop): Dropout2d (p=0.5)\n",
       "  (conv2): Conv2d(20, 40, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d (p=0.5)\n",
       "  (fc1): Linear (640 -> 120)\n",
       "  (fc1_drop): Dropout (p = 0.5)\n",
       "  (fc2): Linear (120 -> 10)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n",
    "        self.conv1_drop = nn.Dropout2d()\n",
    "        self.conv2 = nn.Conv2d(20, 40, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(640, 120)\n",
    "        self.fc1_drop = nn.Dropout()\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1_drop(self.conv1(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1_drop(self.fc1(x)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "torch.Size([20, 1, 5, 5])\n",
      "torch.Size([40, 20, 5, 5])\n",
      "torch.Size([120, 640])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())\n",
    "print(params[2].size())\n",
    "print(params[4].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    net.train()\n",
    "    for batch_index, (inputs, labels) in enumerate(trainloader):\n",
    "        inputs, labels = Variable(inputs), Variable(torch.squeeze(labels))\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = F.nll_loss(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 400 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_index * len(inputs), len(trainloader.dataset),\n",
    "                    100. * batch_index / len(trainloader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in testloader:\n",
    "        data, target = Variable(data, volatile=True), Variable(torch.squeeze(target))\n",
    "        output = net(data)\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(testloader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/25200 (0%)]\tLoss: 2.276732\n",
      "Train Epoch: 0 [3200/25200 (13%)]\tLoss: 0.929732\n",
      "Train Epoch: 0 [6400/25200 (25%)]\tLoss: 1.183684\n",
      "Train Epoch: 0 [9600/25200 (38%)]\tLoss: 0.338008\n",
      "Train Epoch: 0 [12800/25200 (51%)]\tLoss: 1.269049\n",
      "Train Epoch: 0 [16000/25200 (63%)]\tLoss: 1.211067\n",
      "Train Epoch: 0 [19200/25200 (76%)]\tLoss: 0.193084\n",
      "Train Epoch: 0 [22400/25200 (89%)]\tLoss: 1.228068\n",
      "\n",
      "Test set: Average loss: 0.1349, Accuracy: 22137/23070 (96%)\n",
      "\n",
      "Train Epoch: 1 [0/25200 (0%)]\tLoss: 0.308205\n",
      "Train Epoch: 1 [3200/25200 (13%)]\tLoss: 0.129992\n",
      "Train Epoch: 1 [6400/25200 (25%)]\tLoss: 0.515302\n",
      "Train Epoch: 1 [9600/25200 (38%)]\tLoss: 0.029864\n",
      "Train Epoch: 1 [12800/25200 (51%)]\tLoss: 0.036533\n",
      "Train Epoch: 1 [16000/25200 (63%)]\tLoss: 0.447933\n",
      "Train Epoch: 1 [19200/25200 (76%)]\tLoss: 0.178763\n",
      "Train Epoch: 1 [22400/25200 (89%)]\tLoss: 0.418185\n",
      "\n",
      "Test set: Average loss: 0.0897, Accuracy: 22428/23070 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/25200 (0%)]\tLoss: 0.777757\n",
      "Train Epoch: 2 [3200/25200 (13%)]\tLoss: 0.093998\n",
      "Train Epoch: 2 [6400/25200 (25%)]\tLoss: 0.288818\n",
      "Train Epoch: 2 [9600/25200 (38%)]\tLoss: 0.005807\n",
      "Train Epoch: 2 [12800/25200 (51%)]\tLoss: 0.252997\n",
      "Train Epoch: 2 [16000/25200 (63%)]\tLoss: 0.098950\n",
      "Train Epoch: 2 [19200/25200 (76%)]\tLoss: 0.022832\n",
      "Train Epoch: 2 [22400/25200 (89%)]\tLoss: 0.050049\n",
      "\n",
      "Test set: Average loss: 0.0823, Accuracy: 22487/23070 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3): # loop over the dataset multiple times\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
